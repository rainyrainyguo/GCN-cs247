{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Experiment runner for the model with knowledge graph attached to interaction data \"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from gcmc.model import RecommenderGAE, RecommenderSideInfoGAE\n",
    "from gcmc.utils import construct_feed_dict\n",
    "\n",
    "\n",
    "#data_utils.py\n",
    "def data_iterator(data, batch_size):\n",
    "    \"\"\"\n",
    "    A simple data iterator from https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "    :param data: list of numpy tensors that need to be randomly batched across their first dimension.\n",
    "    :param batch_size: int, batch_size of data_iterator.\n",
    "    Assumes same first dimension size of all numpy tensors.\n",
    "    :return: iterator over batches of numpy tensors\n",
    "    \"\"\"\n",
    "    # shuffle labels and features\n",
    "    max_idx = len(data[0])\n",
    "    idxs = np.arange(0, max_idx)\n",
    "    np.random.shuffle(idxs)\n",
    "    shuf_data = [dat[idxs] for dat in data]\n",
    "\n",
    "    # Does not yield last remainder of size less than batch_size\n",
    "    for i in range(max_idx//batch_size):\n",
    "        data_batch = [dat[i*batch_size:(i+1)*batch_size] for dat in shuf_data]\n",
    "        yield data_batch\n",
    "\n",
    "\n",
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    #data = np.array(map(lambda x: id_dict[x], data))\n",
    "    data = pd.Series(data).apply(lambda x: id_dict[x]).values\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "\n",
    "def download_dataset(dataset, files, data_dir):\n",
    "    \"\"\" Downloads dataset if files are not present. \"\"\"\n",
    "\n",
    "    if not np.all([os.path.isfile(data_dir + f) for f in files]):\n",
    "        url = \"http://files.grouplens.org/datasets/movielens/\" + dataset.replace('_', '-') + '.zip'\n",
    "        request = urlopen(url)\n",
    "\n",
    "        print('Downloading %s dataset' % dataset)\n",
    "        if dataset in ['ml_100k', 'ml_1m']:\n",
    "            target_dir = 'data/' + dataset.replace('_', '-')\n",
    "        elif dataset == 'ml_10m':\n",
    "            target_dir = 'data/' + 'ml-10M100K'\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "        with ZipFile(StringIO(request.read())) as zip_ref:\n",
    "            zip_ref.extractall('data/')\n",
    "\n",
    "        source = [target_dir + '/' + s for s in os.listdir(target_dir)]\n",
    "        destination = data_dir+'/'\n",
    "        for f in source:\n",
    "            shutil.copy(f, destination)\n",
    "\n",
    "        shutil.rmtree(target_dir)\n",
    "\n",
    "\n",
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "\n",
    "    num_items : int\n",
    "\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "\n",
    "    print('Loading dataset', fname)\n",
    "\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    if fname == 'ml_100k':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/u.data', '/u.item', '/u.user']\n",
    "\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = '\\t'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            filename, sep=sep, header=None,\n",
    "            names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.as_matrix().tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "        ratings = ratings.astype(np.float64)\n",
    "\n",
    "        # Movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = data_dir + files[1]\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # User features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age']\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing.py\n",
    "def normalize_features(feat):\n",
    "\n",
    "    degree = np.asarray(feat.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv = 1. / degree\n",
    "    degree_inv_mat = sp.diags([degree_inv], [0])\n",
    "    feat_norm = degree_inv_mat.dot(feat)\n",
    "\n",
    "    if feat_norm.nnz == 0:\n",
    "        print('ERROR: normalized adjacency matrix has only zero entries!!!!!')\n",
    "        exit\n",
    "\n",
    "    return feat_norm\n",
    "\n",
    "\n",
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file path\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir = np.asarray(ds['ir'])\n",
    "            jc = np.asarray(ds['jc'])\n",
    "            out = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_user_item_features(u_features, v_features):\n",
    "    \"\"\"\n",
    "    Creates one big feature matrix out of user features and item features.\n",
    "    Stacks item features under the user features.\n",
    "    \"\"\"\n",
    "\n",
    "    zero_csr_u = sp.csr_matrix((u_features.shape[0], v_features.shape[1]), dtype=u_features.dtype)\n",
    "    zero_csr_v = sp.csr_matrix((v_features.shape[0], u_features.shape[1]), dtype=v_features.dtype)\n",
    "\n",
    "    u_features = sp.hstack([u_features, zero_csr_u], format='csr')\n",
    "    v_features = sp.hstack([zero_csr_v, v_features], format='csr')\n",
    "\n",
    "    return u_features, v_features\n",
    "\n",
    "\n",
    "def globally_normalize_bipartite_adjacency(adjacencies, verbose=False, symmetric=True):\n",
    "    \"\"\" Globally Normalizes set of bipartite adjacency matrices \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('Symmetrically normalizing bipartite adj')\n",
    "    # degree_u and degree_v are row and column sums of adj+I\n",
    "\n",
    "    adj_tot = np.sum(adj for adj in adjacencies)\n",
    "    degree_u = np.asarray(adj_tot.sum(1)).flatten()\n",
    "    degree_v = np.asarray(adj_tot.sum(0)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree_u[degree_u == 0.] = np.inf\n",
    "    degree_v[degree_v == 0.] = np.inf\n",
    "\n",
    "    degree_u_inv_sqrt = 1. / np.sqrt(degree_u)\n",
    "    degree_v_inv_sqrt = 1. / np.sqrt(degree_v)\n",
    "    degree_u_inv_sqrt_mat = sp.diags([degree_u_inv_sqrt], [0])\n",
    "    degree_v_inv_sqrt_mat = sp.diags([degree_v_inv_sqrt], [0])\n",
    "\n",
    "    degree_u_inv = degree_u_inv_sqrt_mat.dot(degree_u_inv_sqrt_mat)\n",
    "\n",
    "    if symmetric:\n",
    "        adj_norm = [degree_u_inv_sqrt_mat.dot(adj).dot(degree_v_inv_sqrt_mat) for adj in adjacencies]\n",
    "\n",
    "    else:\n",
    "        adj_norm = [degree_u_inv.dot(adj) for adj in adjacencies]\n",
    "\n",
    "    return adj_norm\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def create_trainvaltest_split(dataset, seed=1234, testing=False, datasplit_path=None, datasplit_from_file=False,\n",
    "                              verbose=True):\n",
    "    \"\"\"\n",
    "    Splits data set into train/val/test sets from full bipartite adjacency matrix. Shuffling of dataset is done in\n",
    "    load_data function.\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if datasplit_from_file and os.path.isfile(datasplit_path):\n",
    "        print('Reading dataset splits from file...')\n",
    "        with open(datasplit_path) as f:\n",
    "            num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pickle.load(f)\n",
    "\n",
    "        if verbose:\n",
    "            print('Number of users = %d' % num_users)\n",
    "            print('Number of items = %d' % num_items)\n",
    "            print('Number of links = %d' % ratings.shape[0])\n",
    "            print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    else:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed,\n",
    "                                                                                            verbose=verbose)\n",
    "\n",
    "        with open(datasplit_path, 'w') as f:\n",
    "            pickle.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)\n",
    "\n",
    "    neutral_rating = -1\n",
    "\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "    num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "    if dataset == 'ml_100k':\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "    else:\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "\n",
    "    num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    train_idx = idx_nonzero[0:num_train]\n",
    "    val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    train_pairs_idx = pairs_nonzero[0:num_train]\n",
    "    val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values\n",
    "\n",
    "\n",
    "def load_official_trainvaltest_split(dataset, testing=False):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "    fname = dataset\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    download_dataset(fname, files, data_dir)\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train = 'data/' + dataset + '/u1.base'\n",
    "    filename_test = 'data/' + dataset + '/u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_array_train = data_train.as_matrix().tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.as_matrix().tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    if dataset =='ml_100k':\n",
    "\n",
    "        # movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = 'data/' + dataset + '/u.item'\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # user features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = 'data/' + dataset + '/u.user'\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        age = users_df['age'].values\n",
    "        age_max = age.max()\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'dataset': 'ml_100k', 'learning_rate': 0.01, 'epochs': 1000, 'hidden': [500, 75], 'feat_hidden': 10, 'accumulation': 'stack', 'dropout': 0.7, 'num_basis_functions': 2, 'data_seed': 1234, 'summaries_dir': 'logs/2020-05-31_09:02:14.912270', 'norm_symmetric': False, 'features': True, 'write_summary': False, 'testing': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "# seed = 123 # use only for unit testing\n",
    "seed = int(time.time())\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:463: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:465: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (943, 23)\n",
      "Item features shape: (1682, 18)\n",
      "Normalizing feature vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:312: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "DATASET = args['dataset']\n",
    "DATASEED = args['data_seed']\n",
    "NB_EPOCH = args['epochs']\n",
    "DO = args['dropout']\n",
    "HIDDEN = args['hidden']\n",
    "FEATHIDDEN = args['feat_hidden']\n",
    "BASES = args['num_basis_functions']\n",
    "LR = args['learning_rate']\n",
    "WRITESUMMARY = args['write_summary']\n",
    "SUMMARIESDIR = args['summaries_dir']\n",
    "FEATURES = args['features']\n",
    "SYM = args['norm_symmetric']\n",
    "TESTING = args['testing']\n",
    "ACCUM = args['accumulation']\n",
    "\n",
    "SELFCONNECTIONS = False\n",
    "SPLITFROMFILE = True\n",
    "VERBOSE = True\n",
    "\n",
    "if DATASET == 'ml_1m' or DATASET == 'ml_100k' or DATASET == 'douban':\n",
    "    NUMCLASSES = 5\n",
    "elif DATASET == 'ml_10m':\n",
    "    NUMCLASSES = 10\n",
    "    print('\\n WARNING: this might run out of RAM, consider using train_minibatch.py for dataset %s' % DATASET)\n",
    "    print('If you want to proceed with this option anyway, uncomment this.\\n')\n",
    "    sys.exit(1)\n",
    "elif DATASET == 'flixster':\n",
    "    NUMCLASSES = 10\n",
    "elif DATASET == 'yahoo_music':\n",
    "    NUMCLASSES = 71\n",
    "    if ACCUM == 'sum':\n",
    "        print('\\n WARNING: combining DATASET=%s with ACCUM=%s can cause memory issues due to large number of classes.')\n",
    "        print('Consider using \"--accum stack\" as an option for this dataset.')\n",
    "        print('If you want to proceed with this option anyway, uncomment this.\\n')\n",
    "        sys.exit(1)\n",
    "\n",
    "# Splitting dataset in training, validation and test set\n",
    "\n",
    "if DATASET == 'ml_1m' or DATASET == 'ml_10m':\n",
    "    if FEATURES:\n",
    "        datasplit_path = 'data/' + DATASET + '/withfeatures_split_seed' + str(DATASEED) + '.pickle'\n",
    "    else:\n",
    "        datasplit_path = 'data/' + DATASET + '/split_seed' + str(DATASEED) + '.pickle'\n",
    "elif FEATURES:\n",
    "    datasplit_path = 'data/' + DATASET + '/withfeatures.pickle'\n",
    "else:\n",
    "    datasplit_path = 'data/' + DATASET + '/nofeatures.pickle'\n",
    "\n",
    "\n",
    "if DATASET == 'flixster' or DATASET == 'douban' or DATASET == 'yahoo_music':\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = load_data_monti(DATASET, TESTING)\n",
    "\n",
    "elif DATASET == 'ml_100k':\n",
    "    print(\"Using official MovieLens dataset split u1.base/u1.test with 20% validation set size...\")\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = load_official_trainvaltest_split(DATASET, TESTING)\n",
    "else:\n",
    "    print(\"Using random dataset split ...\")\n",
    "    u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "        val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "        test_u_indices, test_v_indices, class_values = create_trainvaltest_split(DATASET, DATASEED, TESTING,\n",
    "                                                                                 datasplit_path, SPLITFROMFILE,\n",
    "                                                                                 VERBOSE)\n",
    "\n",
    "num_users, num_items = adj_train.shape\n",
    "\n",
    "num_side_features = 0\n",
    "\n",
    "# feature loading\n",
    "if not FEATURES:\n",
    "    u_features = sp.identity(num_users, format='csr')\n",
    "    v_features = sp.identity(num_items, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(u_features, v_features)\n",
    "\n",
    "elif FEATURES and u_features is not None and v_features is not None:\n",
    "    # use features as side information and node_id's as node input features\n",
    "\n",
    "    print(\"Normalizing feature vectors...\")\n",
    "    u_features_side = normalize_features(u_features)\n",
    "    v_features_side = normalize_features(v_features)\n",
    "\n",
    "    u_features_side, v_features_side = preprocess_user_item_features(u_features_side, v_features_side)\n",
    "\n",
    "    u_features_side = np.array(u_features_side.todense(), dtype=np.float32)\n",
    "    v_features_side = np.array(v_features_side.todense(), dtype=np.float32)\n",
    "\n",
    "    num_side_features = u_features_side.shape[1]\n",
    "\n",
    "    # node id's for node input features\n",
    "    id_csr_v = sp.identity(num_items, format='csr')\n",
    "    id_csr_u = sp.identity(num_users, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(id_csr_u, id_csr_v)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Features flag is set to true but no features are loaded from dataset ' + DATASET)\n",
    "\n",
    "\n",
    "# global normalization\n",
    "support = []\n",
    "support_t = []\n",
    "adj_train_int = sp.csr_matrix(adj_train, dtype=np.int32)\n",
    "\n",
    "for i in range(NUMCLASSES):\n",
    "    # build individual binary rating matrices (supports) for each rating\n",
    "    support_unnormalized = sp.csr_matrix(adj_train_int == i + 1, dtype=np.float32)\n",
    "\n",
    "    if support_unnormalized.nnz == 0 and DATASET != 'yahoo_music':\n",
    "        # yahoo music has dataset split with not all ratings types present in training set.\n",
    "        # this produces empty adjacency matrices for these ratings.\n",
    "        sys.exit('ERROR: normalized bipartite adjacency matrix has only zero entries!!!!!')\n",
    "\n",
    "    support_unnormalized_transpose = support_unnormalized.T\n",
    "    support.append(support_unnormalized)\n",
    "    support_t.append(support_unnormalized_transpose)\n",
    "\n",
    "\n",
    "support = globally_normalize_bipartite_adjacency(support, symmetric=SYM)\n",
    "support_t = globally_normalize_bipartite_adjacency(support_t, symmetric=SYM)\n",
    "\n",
    "if SELFCONNECTIONS:\n",
    "    support.append(sp.identity(u_features.shape[0], format='csr'))\n",
    "    support_t.append(sp.identity(v_features.shape[0], format='csr'))\n",
    "\n",
    "num_support = len(support)\n",
    "support = sp.hstack(support, format='csr')\n",
    "support_t = sp.hstack(support_t, format='csr')\n",
    "\n",
    "if ACCUM == 'stack':\n",
    "    div = HIDDEN[0] // num_support\n",
    "    if HIDDEN[0] % num_support != 0:\n",
    "        print(\"\"\"\\nWARNING: HIDDEN[0] (=%d) of stack layer is adjusted to %d such that\n",
    "                  it can be evenly split in %d splits.\\n\"\"\" % (HIDDEN[0], num_support * div, num_support))\n",
    "    HIDDEN[0] = num_support * div\n",
    "\n",
    "# Collect all user and item nodes for test set\n",
    "test_u = list(set(test_u_indices))\n",
    "test_v = list(set(test_v_indices))\n",
    "test_u_dict = {n: i for i, n in enumerate(test_u)}\n",
    "test_v_dict = {n: i for i, n in enumerate(test_v)}\n",
    "\n",
    "test_u_indices = np.array([test_u_dict[o] for o in test_u_indices])\n",
    "test_v_indices = np.array([test_v_dict[o] for o in test_v_indices])\n",
    "\n",
    "test_support = support[np.array(test_u)]\n",
    "test_support_t = support_t[np.array(test_v)]\n",
    "\n",
    "# Collect all user and item nodes for validation set\n",
    "val_u = list(set(val_u_indices))\n",
    "val_v = list(set(val_v_indices))\n",
    "val_u_dict = {n: i for i, n in enumerate(val_u)}\n",
    "val_v_dict = {n: i for i, n in enumerate(val_v)}\n",
    "\n",
    "val_u_indices = np.array([val_u_dict[o] for o in val_u_indices])\n",
    "val_v_indices = np.array([val_v_dict[o] for o in val_v_indices])\n",
    "\n",
    "val_support = support[np.array(val_u)]\n",
    "val_support_t = support_t[np.array(val_v)]\n",
    "\n",
    "# Collect all user and item nodes for train set\n",
    "train_u = list(set(train_u_indices))\n",
    "train_v = list(set(train_v_indices))\n",
    "train_u_dict = {n: i for i, n in enumerate(train_u)}\n",
    "train_v_dict = {n: i for i, n in enumerate(train_v)}\n",
    "\n",
    "train_u_indices = np.array([train_u_dict[o] for o in train_u_indices])\n",
    "train_v_indices = np.array([train_v_dict[o] for o in train_v_indices])\n",
    "\n",
    "train_support = support[np.array(train_u)]\n",
    "train_support_t = support_t[np.array(train_v)]\n",
    "\n",
    "# features as side info\n",
    "if FEATURES:\n",
    "    test_u_features_side = u_features_side[np.array(test_u)]\n",
    "    test_v_features_side = v_features_side[np.array(test_v)]\n",
    "\n",
    "    val_u_features_side = u_features_side[np.array(val_u)]\n",
    "    val_v_features_side = v_features_side[np.array(val_v)]\n",
    "\n",
    "    train_u_features_side = u_features_side[np.array(train_u)]\n",
    "    train_v_features_side = v_features_side[np.array(train_v)]\n",
    "\n",
    "else:\n",
    "    test_u_features_side = None\n",
    "    test_v_features_side = None\n",
    "\n",
    "    val_u_features_side = None\n",
    "    val_v_features_side = None\n",
    "\n",
    "    train_u_features_side = None\n",
    "    train_v_features_side = None\n",
    "\n",
    "placeholders = {\n",
    "    'u_features': tf.sparse_placeholder(tf.float32, shape=np.array(u_features.shape, dtype=np.int64)),\n",
    "    'v_features': tf.sparse_placeholder(tf.float32, shape=np.array(v_features.shape, dtype=np.int64)),\n",
    "    'u_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'v_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'labels': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'u_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "    'v_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "\n",
    "    'user_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "    'item_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'class_values': tf.placeholder(tf.float32, shape=class_values.shape),\n",
    "\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'weight_decay': tf.placeholder_with_default(0., shape=()),\n",
    "\n",
    "    'support': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "    'support_t': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "}\n",
    "\n",
    "# create model\n",
    "if FEATURES:\n",
    "    model = RecommenderSideInfoGAE(placeholders,\n",
    "                                   input_dim=u_features.shape[1],\n",
    "                                   feat_hidden_dim=FEATHIDDEN,\n",
    "                                   num_classes=NUMCLASSES,\n",
    "                                   num_support=num_support,\n",
    "                                   self_connections=SELFCONNECTIONS,\n",
    "                                   num_basis_functions=BASES,\n",
    "                                   hidden=HIDDEN,\n",
    "                                   num_users=num_users,\n",
    "                                   num_items=num_items,\n",
    "                                   accum=ACCUM,\n",
    "                                   learning_rate=LR,\n",
    "                                   num_side_features=num_side_features,\n",
    "                                   logging=True)\n",
    "\n",
    "# Convert sparse placeholders to tuples to construct feed_dict\n",
    "test_support = sparse_to_tuple(test_support)\n",
    "test_support_t = sparse_to_tuple(test_support_t)\n",
    "\n",
    "val_support = sparse_to_tuple(val_support)\n",
    "val_support_t = sparse_to_tuple(val_support_t)\n",
    "\n",
    "train_support = sparse_to_tuple(train_support)\n",
    "train_support_t = sparse_to_tuple(train_support_t)\n",
    "\n",
    "u_features = sparse_to_tuple(u_features)\n",
    "v_features = sparse_to_tuple(v_features)\n",
    "assert u_features[2][1] == v_features[2][1], 'Number of features of users and items must be the same!'\n",
    "\n",
    "num_features = u_features[2][1]\n",
    "u_features_nonzero = u_features[1].shape[0]\n",
    "v_features_nonzero = v_features[1].shape[0]\n",
    "\n",
    "# Feed_dicts for validation and test set stay constant over different update steps\n",
    "train_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                      v_features_nonzero, train_support, train_support_t,\n",
    "                                      train_labels, train_u_indices, train_v_indices, class_values, DO,\n",
    "                                      train_u_features_side, train_v_features_side)\n",
    "# No dropout for validation and test runs\n",
    "val_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                    v_features_nonzero, val_support, val_support_t,\n",
    "                                    val_labels, val_u_indices, val_v_indices, class_values, 0.,\n",
    "                                    val_u_features_side, val_v_features_side)\n",
    "\n",
    "test_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                     v_features_nonzero, test_support, test_support_t,\n",
    "                                     test_labels, test_u_indices, test_v_indices, class_values, 0.,\n",
    "                                     test_u_features_side, test_v_features_side)\n",
    "\n",
    "\n",
    "# Collect all variables to be logged into summary\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if WRITESUMMARY:\n",
    "    train_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/train', sess.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/val')\n",
    "else:\n",
    "    train_summary_writer = None\n",
    "    val_summary_writer = None\n",
    "\n",
    "best_val_score = np.inf\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[*] Epoch: 0001 train_loss= 1.61377 train_rmse= 1.24167 val_loss= 1.59396 val_rmse= 1.17648 \t\ttime= 0.29260\n",
      "[*] Epoch: 0002 train_loss= 1.61055 train_rmse= 1.19002 val_loss= 1.59169 val_rmse= 1.19842 \t\ttime= 0.21654\n",
      "[*] Epoch: 0003 train_loss= 1.60251 train_rmse= 1.20341 val_loss= 1.59457 val_rmse= 1.21302 \t\ttime= 0.19545\n",
      "[*] Epoch: 0004 train_loss= 1.59920 train_rmse= 1.21490 val_loss= 1.58926 val_rmse= 1.20130 \t\ttime= 0.20292\n",
      "[*] Epoch: 0005 train_loss= 1.59323 train_rmse= 1.20299 val_loss= 1.57699 val_rmse= 1.17042 \t\ttime= 0.19936\n",
      "[*] Epoch: 0006 train_loss= 1.58572 train_rmse= 1.17616 val_loss= 1.56257 val_rmse= 1.13328 \t\ttime= 0.18648\n",
      "[*] Epoch: 0007 train_loss= 1.57356 train_rmse= 1.14536 val_loss= 1.54442 val_rmse= 1.10284 \t\ttime= 0.21175\n",
      "[*] Epoch: 0008 train_loss= 1.55863 train_rmse= 1.11967 val_loss= 1.52247 val_rmse= 1.08566 \t\ttime= 0.19439\n",
      "[*] Epoch: 0009 train_loss= 1.53866 train_rmse= 1.10108 val_loss= 1.50483 val_rmse= 1.06942 \t\ttime= 0.19252\n",
      "[*] Epoch: 0010 train_loss= 1.53288 train_rmse= 1.09787 val_loss= 1.49088 val_rmse= 1.01840 \t\ttime= 0.21724\n",
      "[*] Epoch: 0011 train_loss= 1.51590 train_rmse= 1.04717 val_loss= 1.48183 val_rmse= 1.00278 \t\ttime= 0.20825\n",
      "[*] Epoch: 0012 train_loss= 1.51396 train_rmse= 1.01297 val_loss= 1.47819 val_rmse= 1.04588 \t\ttime= 0.20040\n",
      "[*] Epoch: 0013 train_loss= 1.49540 train_rmse= 1.06360 val_loss= 1.47824 val_rmse= 1.05949 \t\ttime= 0.19027\n",
      "[*] Epoch: 0014 train_loss= 1.48708 train_rmse= 1.06405 val_loss= 1.46195 val_rmse= 1.02370 \t\ttime= 0.18834\n",
      "[*] Epoch: 0015 train_loss= 1.47296 train_rmse= 1.02021 val_loss= 1.44765 val_rmse= 0.99455 \t\ttime= 0.20825\n",
      "[*] Epoch: 0016 train_loss= 1.46911 train_rmse= 1.00517 val_loss= 1.43410 val_rmse= 0.98696 \t\ttime= 0.21662\n",
      "[*] Epoch: 0017 train_loss= 1.47244 train_rmse= 1.03917 val_loss= 1.42172 val_rmse= 0.96592 \t\ttime= 0.22051\n",
      "[*] Epoch: 0018 train_loss= 1.45857 train_rmse= 0.99734 val_loss= 1.40950 val_rmse= 0.96110 \t\ttime= 0.18807\n",
      "[*] Epoch: 0019 train_loss= 1.45335 train_rmse= 1.00218 val_loss= 1.40049 val_rmse= 0.96122 \t\ttime= 0.19480\n",
      "[*] Epoch: 0020 train_loss= 1.44472 train_rmse= 0.99339 val_loss= 1.40281 val_rmse= 0.97767 \t\ttime= 0.17365\n",
      "[*] Epoch: 0021 train_loss= 1.44520 train_rmse= 1.03162 val_loss= 1.40236 val_rmse= 0.96703 \t\ttime= 0.22923\n",
      "[*] Epoch: 0022 train_loss= 1.43264 train_rmse= 0.99491 val_loss= 1.39992 val_rmse= 0.96156 \t\ttime= 0.19781\n",
      "[*] Epoch: 0023 train_loss= 1.42649 train_rmse= 0.98196 val_loss= 1.39159 val_rmse= 0.96484 \t\ttime= 0.19746\n",
      "[*] Epoch: 0024 train_loss= 1.42377 train_rmse= 0.98574 val_loss= 1.38078 val_rmse= 0.97337 \t\ttime= 0.20122\n",
      "[*] Epoch: 0025 train_loss= 1.41031 train_rmse= 0.99482 val_loss= 1.37011 val_rmse= 0.96777 \t\ttime= 0.18163\n",
      "[*] Epoch: 0026 train_loss= 1.40942 train_rmse= 1.00071 val_loss= 1.36109 val_rmse= 0.94673 \t\ttime= 0.20483\n",
      "[*] Epoch: 0027 train_loss= 1.40034 train_rmse= 0.96454 val_loss= 1.36168 val_rmse= 0.94795 \t\ttime= 0.21101\n",
      "[*] Epoch: 0028 train_loss= 1.38887 train_rmse= 0.97074 val_loss= 1.36182 val_rmse= 0.95287 \t\ttime= 0.20292\n",
      "[*] Epoch: 0029 train_loss= 1.40035 train_rmse= 0.99463 val_loss= 1.34881 val_rmse= 0.94534 \t\ttime= 0.20495\n",
      "[*] Epoch: 0030 train_loss= 1.37332 train_rmse= 0.96747 val_loss= 1.33433 val_rmse= 0.94225 \t\ttime= 0.22261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dd95caa400c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# outs = sess.run([model.opt_op, model.loss, model.rmse], feed_dict=train_feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# with exponential moving averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    # Run single weight update\n",
    "    # outs = sess.run([model.opt_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "    # with exponential moving averages\n",
    "    outs = sess.run([model.training_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "\n",
    "    train_avg_loss = outs[1]\n",
    "    train_rmse = outs[2]\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"[*] Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_avg_loss),\n",
    "              \"train_rmse=\", \"{:.5f}\".format(train_rmse),\n",
    "              \"val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"val_rmse=\", \"{:.5f}\".format(val_rmse),\n",
    "              \"\\t\\ttime=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if val_rmse < best_val_score:\n",
    "        best_val_score = val_rmse\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 20 == 0 and WRITESUMMARY:\n",
    "        # Train set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=train_feed_dict)\n",
    "        train_summary_writer.add_summary(summary, epoch)\n",
    "        train_summary_writer.flush()\n",
    "\n",
    "        # Validation set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=val_feed_dict)\n",
    "        val_summary_writer.add_summary(summary, epoch)\n",
    "        val_summary_writer.flush()\n",
    "\n",
    "    if epoch % 100 == 0 and epoch > 1000 and not TESTING and False:\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"tmp/%s_seed%d.ckpt\" % (model.name, DATASEED), global_step=model.global_step)\n",
    "\n",
    "        # load polyak averages\n",
    "        variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "        val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "        print('polyak val loss = ', val_avg_loss)\n",
    "        print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "        # Load back normal variables\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "\n",
    "# store model including exponential moving averages\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"tmp/%s.ckpt\" % model.name, global_step=model.global_step)\n",
    "\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"\\nOptimization Finished!\")\n",
    "    print('best validation score =', best_val_score, 'at iteration', best_epoch)\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('test loss = ', test_avg_loss)\n",
    "    print('test rmse = ', test_rmse)\n",
    "\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('polyak test loss = ', test_avg_loss)\n",
    "    print('polyak test rmse = ', test_rmse)\n",
    "\n",
    "else:\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "    print('polyak val loss = ', val_avg_loss)\n",
    "    print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "print('\\nSETTINGS:\\n')\n",
    "for key, val in sorted(vars(ap.parse_args()).items()):\n",
    "    print(key, val)\n",
    "\n",
    "print('global seed = ', seed)\n",
    "\n",
    "# For parsing results from file\n",
    "results = vars(ap.parse_args()).copy()\n",
    "results.update({'best_val_score': float(best_val_score), 'best_epoch': best_epoch})\n",
    "print(json.dumps(results))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
