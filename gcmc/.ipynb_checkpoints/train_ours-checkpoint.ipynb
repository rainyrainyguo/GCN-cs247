{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Experiment runner for the model with knowledge graph attached to interaction data \"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import random\n",
    "from zipfile import ZipFile\n",
    "from io import StringIO\n",
    "import shutil\n",
    "import os.path\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import h5py\n",
    "from urllib.request import urlopen\n",
    "\n",
    "from gcmc.model import RecommenderGAE, RecommenderSideInfoGAE\n",
    "from gcmc.utils import construct_feed_dict\n",
    "\n",
    "\n",
    "#data_utils.py\n",
    "def data_iterator(data, batch_size):\n",
    "    \"\"\"\n",
    "    A simple data iterator from https://indico.io/blog/tensorflow-data-inputs-part1-placeholders-protobufs-queues/\n",
    "    :param data: list of numpy tensors that need to be randomly batched across their first dimension.\n",
    "    :param batch_size: int, batch_size of data_iterator.\n",
    "    Assumes same first dimension size of all numpy tensors.\n",
    "    :return: iterator over batches of numpy tensors\n",
    "    \"\"\"\n",
    "    # shuffle labels and features\n",
    "    max_idx = len(data[0])\n",
    "    idxs = np.arange(0, max_idx)\n",
    "    np.random.shuffle(idxs)\n",
    "    shuf_data = [dat[idxs] for dat in data]\n",
    "\n",
    "    # Does not yield last remainder of size less than batch_size\n",
    "    for i in range(max_idx//batch_size):\n",
    "        data_batch = [dat[i*batch_size:(i+1)*batch_size] for dat in shuf_data]\n",
    "        yield data_batch\n",
    "\n",
    "\n",
    "def map_data(data):\n",
    "    \"\"\"\n",
    "    Map data to proper indices in case they are not in a continues [0, N) range\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : np.int32 arrays\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapped_data : np.int32 arrays\n",
    "    n : length of mapped_data\n",
    "\n",
    "    \"\"\"\n",
    "    uniq = list(set(data))\n",
    "\n",
    "    id_dict = {old: new for new, old in enumerate(sorted(uniq))}\n",
    "    #data = np.array(map(lambda x: id_dict[x], data))\n",
    "    data = pd.Series(data).apply(lambda x: id_dict[x]).values\n",
    "    n = len(uniq)\n",
    "\n",
    "    return data, id_dict, n\n",
    "\n",
    "\n",
    "def download_dataset(dataset, files, data_dir):\n",
    "    \"\"\" Downloads dataset if files are not present. \"\"\"\n",
    "\n",
    "    if not np.all([os.path.isfile(data_dir + f) for f in files]):\n",
    "        url = \"http://files.grouplens.org/datasets/movielens/\" + dataset.replace('_', '-') + '.zip'\n",
    "        request = urlopen(url)\n",
    "\n",
    "        print('Downloading %s dataset' % dataset)\n",
    "        if dataset in ['ml_100k', 'ml_1m']:\n",
    "            target_dir = 'data/' + dataset.replace('_', '-')\n",
    "        elif dataset == 'ml_10m':\n",
    "            target_dir = 'data/' + 'ml-10M100K'\n",
    "        else:\n",
    "            raise ValueError('Invalid dataset option %s' % dataset)\n",
    "\n",
    "        with ZipFile(StringIO(request.read())) as zip_ref:\n",
    "            zip_ref.extractall('data/')\n",
    "\n",
    "        source = [target_dir + '/' + s for s in os.listdir(target_dir)]\n",
    "        destination = data_dir+'/'\n",
    "        for f in source:\n",
    "            shutil.copy(f, destination)\n",
    "\n",
    "        shutil.rmtree(target_dir)\n",
    "\n",
    "\n",
    "def load_data(fname, seed=1234, verbose=True):\n",
    "    \"\"\" Loads dataset and creates adjacency matrix\n",
    "    and feature matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fname : str, dataset\n",
    "    seed: int, dataset shuffling seed\n",
    "    verbose: to print out statements or not\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    num_users : int\n",
    "        Number of users and items respectively\n",
    "\n",
    "    num_items : int\n",
    "\n",
    "    u_nodes : np.int32 arrays\n",
    "        User indices\n",
    "\n",
    "    v_nodes : np.int32 array\n",
    "        item (movie) indices\n",
    "\n",
    "    ratings : np.float32 array\n",
    "        User/item ratings s.t. ratings[k] is the rating given by user u_nodes[k] to\n",
    "        item v_nodes[k]. Note that that the all pairs u_nodes[k]/v_nodes[k] are unique, but\n",
    "        not necessarily all u_nodes[k] or all v_nodes[k] separately.\n",
    "\n",
    "    u_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    v_features: np.float32 array, or None\n",
    "        If present in dataset, contains the features of the users.\n",
    "\n",
    "    seed: int,\n",
    "        For datashuffling seed with pythons own random.shuffle, as in CF-NADE.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    u_features = None\n",
    "    v_features = None\n",
    "\n",
    "    print('Loading dataset', fname)\n",
    "\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    if fname == 'ml_100k':\n",
    "\n",
    "        # Check if files exist and download otherwise\n",
    "        files = ['/u.data', '/u.item', '/u.user']\n",
    "\n",
    "        download_dataset(fname, files, data_dir)\n",
    "\n",
    "        sep = '\\t'\n",
    "        filename = data_dir + files[0]\n",
    "\n",
    "        dtypes = {\n",
    "            'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "            'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "        data = pd.read_csv(\n",
    "            filename, sep=sep, header=None,\n",
    "            names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "        # shuffle here like cf-nade paper with python's own random class\n",
    "        # make sure to convert to list, otherwise random.shuffle acts weird on it without a warning\n",
    "        data_array = data.as_matrix().tolist()\n",
    "        random.seed(seed)\n",
    "        random.shuffle(data_array)\n",
    "        data_array = np.array(data_array)\n",
    "\n",
    "        u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "        v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "        ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "        u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "        v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "        u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "        ratings = ratings.astype(np.float64)\n",
    "\n",
    "        # Movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = data_dir + files[1]\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # Check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # User features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = data_dir + files[2]\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age']\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "        u_features = sp.csr_matrix(u_features)\n",
    "        v_features = sp.csr_matrix(v_features)\n",
    "        \n",
    "\n",
    "    if verbose:\n",
    "        print('Number of users = %d' % num_users)\n",
    "        print('Number of items = %d' % num_items)\n",
    "        print('Number of links = %d' % ratings.shape[0])\n",
    "        print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    return num_users, num_items, u_nodes_ratings, v_nodes_ratings, ratings, u_features, v_features\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing.py\n",
    "def normalize_features(feat):\n",
    "\n",
    "    degree = np.asarray(feat.sum(1)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree[degree == 0.] = np.inf\n",
    "\n",
    "    degree_inv = 1. / degree\n",
    "    degree_inv_mat = sp.diags([degree_inv], [0])\n",
    "    feat_norm = degree_inv_mat.dot(feat)\n",
    "\n",
    "    if feat_norm.nnz == 0:\n",
    "        print('ERROR: normalized adjacency matrix has only zero entries!!!!!')\n",
    "        exit\n",
    "\n",
    "    return feat_norm\n",
    "\n",
    "\n",
    "def load_matlab_file(path_file, name_field):\n",
    "    \"\"\"\n",
    "    load '.mat' files\n",
    "    inputs:\n",
    "        path_file, string containing the file path\n",
    "        name_field, string containig the field name (default='shape')\n",
    "    warning:\n",
    "        '.mat' files should be saved in the '-v7.3' format\n",
    "    \"\"\"\n",
    "    db = h5py.File(path_file, 'r')\n",
    "    ds = db[name_field]\n",
    "    try:\n",
    "        if 'ir' in ds.keys():\n",
    "            data = np.asarray(ds['data'])\n",
    "            ir = np.asarray(ds['ir'])\n",
    "            jc = np.asarray(ds['jc'])\n",
    "            out = sp.csc_matrix((data, ir, jc)).astype(np.float32)\n",
    "    except AttributeError:\n",
    "        # Transpose in case is a dense matrix because of the row- vs column- major ordering between python and matlab\n",
    "        out = np.asarray(ds).astype(np.float32).T\n",
    "\n",
    "    db.close()\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def preprocess_user_item_features(u_features, v_features):\n",
    "    \"\"\"\n",
    "    Creates one big feature matrix out of user features and item features.\n",
    "    Stacks item features under the user features.\n",
    "    \"\"\"\n",
    "\n",
    "    zero_csr_u = sp.csr_matrix((u_features.shape[0], v_features.shape[1]), dtype=u_features.dtype)\n",
    "    zero_csr_v = sp.csr_matrix((v_features.shape[0], u_features.shape[1]), dtype=v_features.dtype)\n",
    "\n",
    "    u_features = sp.hstack([u_features, zero_csr_u], format='csr')\n",
    "    v_features = sp.hstack([zero_csr_v, v_features], format='csr')\n",
    "\n",
    "    return u_features, v_features\n",
    "\n",
    "\n",
    "def globally_normalize_bipartite_adjacency(adjacencies, verbose=False, symmetric=True):\n",
    "    \"\"\" Globally Normalizes set of bipartite adjacency matrices \"\"\"\n",
    "\n",
    "    if verbose:\n",
    "        print('Symmetrically normalizing bipartite adj')\n",
    "    # degree_u and degree_v are row and column sums of adj+I\n",
    "\n",
    "    adj_tot = np.sum(adj for adj in adjacencies)\n",
    "    degree_u = np.asarray(adj_tot.sum(1)).flatten()\n",
    "    degree_v = np.asarray(adj_tot.sum(0)).flatten()\n",
    "\n",
    "    # set zeros to inf to avoid dividing by zero\n",
    "    degree_u[degree_u == 0.] = np.inf\n",
    "    degree_v[degree_v == 0.] = np.inf\n",
    "\n",
    "    degree_u_inv_sqrt = 1. / np.sqrt(degree_u)\n",
    "    degree_v_inv_sqrt = 1. / np.sqrt(degree_v)\n",
    "    degree_u_inv_sqrt_mat = sp.diags([degree_u_inv_sqrt], [0])\n",
    "    degree_v_inv_sqrt_mat = sp.diags([degree_v_inv_sqrt], [0])\n",
    "\n",
    "    degree_u_inv = degree_u_inv_sqrt_mat.dot(degree_u_inv_sqrt_mat)\n",
    "\n",
    "    if symmetric:\n",
    "        adj_norm = [degree_u_inv_sqrt_mat.dot(adj).dot(degree_v_inv_sqrt_mat) for adj in adjacencies]\n",
    "\n",
    "    else:\n",
    "        adj_norm = [degree_u_inv.dot(adj) for adj in adjacencies]\n",
    "\n",
    "    return adj_norm\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\" change of format for sparse matrix. This format is used\n",
    "    for the feed_dict where sparse matrices need to be linked to placeholders\n",
    "    representing sparse matrices. \"\"\"\n",
    "\n",
    "    if not sp.isspmatrix_coo(sparse_mx):\n",
    "        sparse_mx = sparse_mx.tocoo()\n",
    "    coords = np.vstack((sparse_mx.row, sparse_mx.col)).transpose()\n",
    "    values = sparse_mx.data\n",
    "    shape = sparse_mx.shape\n",
    "    return coords, values, shape\n",
    "\n",
    "\n",
    "def create_trainvaltest_split(dataset, seed=1234, testing=False, datasplit_path=None, datasplit_from_file=False,\n",
    "                              verbose=True):\n",
    "    \"\"\"\n",
    "    Splits data set into train/val/test sets from full bipartite adjacency matrix. Shuffling of dataset is done in\n",
    "    load_data function.\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    if datasplit_from_file and os.path.isfile(datasplit_path):\n",
    "        print('Reading dataset splits from file...')\n",
    "        with open(datasplit_path) as f:\n",
    "            num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = pickle.load(f)\n",
    "\n",
    "        if verbose:\n",
    "            print('Number of users = %d' % num_users)\n",
    "            print('Number of items = %d' % num_items)\n",
    "            print('Number of links = %d' % ratings.shape[0])\n",
    "            print('Fraction of positive links = %.4f' % (float(ratings.shape[0]) / (num_users * num_items),))\n",
    "\n",
    "    else:\n",
    "        num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features = load_data(dataset, seed=seed,\n",
    "                                                                                            verbose=verbose)\n",
    "\n",
    "        with open(datasplit_path, 'w') as f:\n",
    "            pickle.dump([num_users, num_items, u_nodes, v_nodes, ratings, u_features, v_features], f)\n",
    "\n",
    "    neutral_rating = -1\n",
    "\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges\n",
    "    num_test = int(np.ceil(ratings.shape[0] * 0.1))\n",
    "    if dataset == 'ml_100k':\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "    else:\n",
    "        num_val = int(np.ceil(ratings.shape[0] * 0.9 * 0.05))\n",
    "\n",
    "    num_train = ratings.shape[0] - num_val - num_test\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    train_idx = idx_nonzero[0:num_train]\n",
    "    val_idx = idx_nonzero[num_train:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    train_pairs_idx = pairs_nonzero[0:num_train]\n",
    "    val_pairs_idx = pairs_nonzero[num_train:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_official_trainvaltest_split(dataset, testing=False):\n",
    "    \"\"\"\n",
    "    Loads official train/test split and uses 10% of training samples for validaiton\n",
    "    For each split computes 1-of-num_classes labels. Also computes training\n",
    "    adjacency matrix. Assumes flattening happens everywhere in row-major fashion.\n",
    "    \"\"\"\n",
    "\n",
    "    sep = '\\t'\n",
    "\n",
    "    # Check if files exist and download otherwise\n",
    "    files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "    fname = dataset\n",
    "    data_dir = 'data/' + fname\n",
    "\n",
    "    download_dataset(fname, files, data_dir)\n",
    "\n",
    "    dtypes = {\n",
    "        'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "        'ratings': np.float32, 'timestamp': np.float64}\n",
    "\n",
    "    filename_train = 'data/' + dataset + '/u1.base'\n",
    "    filename_test = 'data/' + dataset + '/u1.test'\n",
    "\n",
    "    data_train = pd.read_csv(\n",
    "        filename_train, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "\n",
    "    data_test = pd.read_csv(\n",
    "        filename_test, sep=sep, header=None,\n",
    "        names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)\n",
    "    \n",
    "    ###############################################################################\n",
    "\n",
    "    data_array_train = data_train.as_matrix().tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.as_matrix().tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "    if dataset =='ml_100k':\n",
    "\n",
    "        # movie features (genres)\n",
    "        sep = r'|'\n",
    "        movie_file = 'data/' + dataset + '/u.item'\n",
    "        movie_headers = ['movie id', 'movie title', 'release date', 'video release date',\n",
    "                         'IMDb URL', 'unknown', 'Action', 'Adventure', 'Animation',\n",
    "                         'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                         'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                         'Thriller', 'War', 'Western']\n",
    "        movie_df = pd.read_csv(movie_file, sep=sep, header=None,\n",
    "                               names=movie_headers, engine='python')\n",
    "\n",
    "        genre_headers = movie_df.columns.values[6:]\n",
    "        num_genres = genre_headers.shape[0]\n",
    "\n",
    "        v_features = np.zeros((num_items, num_genres), dtype=np.float32)\n",
    "        for movie_id, g_vec in zip(movie_df['movie id'].values.tolist(), movie_df[genre_headers].values.tolist()):\n",
    "            # check if movie_id was listed in ratings file and therefore in mapping dictionary\n",
    "            if movie_id in v_dict.keys():\n",
    "                v_features[v_dict[movie_id], :] = g_vec\n",
    "\n",
    "        # user features\n",
    "\n",
    "        sep = r'|'\n",
    "        users_file = 'data/' + dataset + '/u.user'\n",
    "        users_headers = ['user id', 'age', 'gender', 'occupation', 'zip code']\n",
    "        users_df = pd.read_csv(users_file, sep=sep, header=None,\n",
    "                               names=users_headers, engine='python')\n",
    "\n",
    "        occupation = set(users_df['occupation'].values.tolist())\n",
    "\n",
    "        age = users_df['age'].values\n",
    "        age_max = age.max()\n",
    "\n",
    "        gender_dict = {'M': 0., 'F': 1.}\n",
    "        occupation_dict = {f: i for i, f in enumerate(occupation, start=2)}\n",
    "\n",
    "        num_feats = 2 + len(occupation_dict)\n",
    "\n",
    "        u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "        for _, row in users_df.iterrows():\n",
    "            u_id = row['user id']\n",
    "            if u_id in u_dict.keys():\n",
    "                # age\n",
    "                u_features[u_dict[u_id], 0] = row['age'] / np.float(age_max)\n",
    "                # gender\n",
    "                u_features[u_dict[u_id], 1] = gender_dict[row['gender']]\n",
    "                # occupation\n",
    "                u_features[u_dict[u_id], occupation_dict[row['occupation']]] = 1.\n",
    "\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {'dataset': 'ml_100k', 'learning_rate': 0.01, 'epochs': 1000, 'hidden': [500, 75], 'feat_hidden': 10, 'accumulation': 'stack', 'dropout': 0.7, 'num_basis_functions': 2, 'data_seed': 1234, 'summaries_dir': 'logs/2020-05-31_09:02:14.912270', 'norm_symmetric': False, 'features': True, 'write_summary': False, 'testing': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "# seed = 123 # use only for unit testing\n",
    "seed = int(time.time())\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ourdata(now_phase = 1, testing = True):\n",
    "    train_path = '../../underexpose_train'  \n",
    "    test_path = '../../underexpose_test'  \n",
    "    recom_item = []  \n",
    "\n",
    "    data_train = pd.DataFrame()  \n",
    "    data_test = pd.DataFrame()\n",
    "    for c in range(now_phase + 1):  \n",
    "        print('phase:', c)  \n",
    "        click_train = pd.read_csv(train_path + '/underexpose_train_click-{}-real.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c,c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "        answer = pd.read_csv(test_path + '/underexpose_test_qtime_with_answer-{}.csv'.format(c), header=None, names=['user_id', 'item_id', 'time'])\n",
    "\n",
    "        all_click = click_train.append(click_test)  \n",
    "        data_train =data_train.append(all_click)  \n",
    "        data_train = data_train.drop_duplicates(subset=['user_id','item_id','time'],keep='last')\n",
    "        #data_train = data_train.sort_values('time')\n",
    "        data_test = data_test.append(answer)\n",
    "\n",
    "    data_train = data_train.reset_index().drop(columns=['index'], axis=1)\n",
    "    data_train.insert(loc=2, column='ratings', value=np.ones(len(data_train)))\n",
    "    data_train.columns = ['u_nodes','v_nodes','ratings','timestamp']\n",
    "\n",
    "    data_test = data_test.reset_index().drop(columns=['index'], axis=1)\n",
    "    data_test.insert(loc=2, column='ratings', value=np.ones(len(data_test)))\n",
    "    data_test.columns = ['u_nodes','v_nodes','ratings','timestamp']\n",
    "\n",
    "    data_array_train = data_train.as_matrix().tolist()\n",
    "    data_array_train = np.array(data_array_train)\n",
    "    data_array_test = data_test.as_matrix().tolist()\n",
    "    data_array_test = np.array(data_array_test)\n",
    "\n",
    "    data_array = np.concatenate([data_array_train, data_array_test], axis=0)\n",
    "\n",
    "    u_nodes_ratings = data_array[:, 0].astype(dtypes['u_nodes'])\n",
    "    v_nodes_ratings = data_array[:, 1].astype(dtypes['v_nodes'])\n",
    "    ratings = data_array[:, 2].astype(dtypes['ratings'])\n",
    "\n",
    "    u_nodes_ratings, u_dict, num_users = map_data(u_nodes_ratings)\n",
    "    v_nodes_ratings, v_dict, num_items = map_data(v_nodes_ratings)\n",
    "\n",
    "    u_nodes_ratings, v_nodes_ratings = u_nodes_ratings.astype(np.int64), v_nodes_ratings.astype(np.int32)\n",
    "    ratings = ratings.astype(np.float64)\n",
    "\n",
    "    u_nodes = u_nodes_ratings\n",
    "    v_nodes = v_nodes_ratings\n",
    "\n",
    "    neutral_rating = -1  # int(np.ceil(np.float(num_classes)/2.)) - 1\n",
    "\n",
    "    # assumes that ratings_train contains at least one example of every rating type\n",
    "    rating_dict = {r: i for i, r in enumerate(np.sort(np.unique(ratings)).tolist())}\n",
    "\n",
    "    labels = np.full((num_users, num_items), neutral_rating, dtype=np.int32)\n",
    "    labels[u_nodes, v_nodes] = np.array([rating_dict[r] for r in ratings])\n",
    "\n",
    "    for i in range(len(u_nodes)):\n",
    "        assert(labels[u_nodes[i], v_nodes[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    labels = labels.reshape([-1])\n",
    "\n",
    "    # number of test and validation edges, see cf-nade code\n",
    "\n",
    "    num_train = data_array_train.shape[0]\n",
    "    num_test = data_array_test.shape[0]\n",
    "    num_val = int(np.ceil(num_train * 0.2))\n",
    "    num_train = num_train - num_val\n",
    "\n",
    "    pairs_nonzero = np.array([[u, v] for u, v in zip(u_nodes, v_nodes)])\n",
    "    idx_nonzero = np.array([u * num_items + v for u, v in pairs_nonzero])\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        assert(labels[idx_nonzero[i]] == rating_dict[ratings[i]])\n",
    "\n",
    "    idx_nonzero_train = idx_nonzero[0:num_train+num_val]\n",
    "    idx_nonzero_test = idx_nonzero[num_train+num_val:]\n",
    "\n",
    "    pairs_nonzero_train = pairs_nonzero[0:num_train+num_val]\n",
    "    pairs_nonzero_test = pairs_nonzero[num_train+num_val:]\n",
    "\n",
    "    # Internally shuffle training set (before splitting off validation set)\n",
    "    rand_idx = list(range(len(idx_nonzero_train)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(rand_idx)\n",
    "    idx_nonzero_train = idx_nonzero_train[rand_idx]\n",
    "    pairs_nonzero_train = pairs_nonzero_train[rand_idx]\n",
    "\n",
    "    idx_nonzero = np.concatenate([idx_nonzero_train, idx_nonzero_test], axis=0)\n",
    "    pairs_nonzero = np.concatenate([pairs_nonzero_train, pairs_nonzero_test], axis=0)\n",
    "\n",
    "    val_idx = idx_nonzero[0:num_val]\n",
    "    train_idx = idx_nonzero[num_val:num_train + num_val]\n",
    "    test_idx = idx_nonzero[num_train + num_val:]\n",
    "\n",
    "    assert(len(test_idx) == num_test)\n",
    "\n",
    "    val_pairs_idx = pairs_nonzero[0:num_val]\n",
    "    train_pairs_idx = pairs_nonzero[num_val:num_train + num_val]\n",
    "    test_pairs_idx = pairs_nonzero[num_train + num_val:]\n",
    "\n",
    "    u_test_idx, v_test_idx = test_pairs_idx.transpose()\n",
    "    u_val_idx, v_val_idx = val_pairs_idx.transpose()\n",
    "    u_train_idx, v_train_idx = train_pairs_idx.transpose()\n",
    "\n",
    "    # create labels\n",
    "    train_labels = labels[train_idx]\n",
    "    val_labels = labels[val_idx]\n",
    "    test_labels = labels[test_idx]\n",
    "\n",
    "    if testing:\n",
    "        u_train_idx = np.hstack([u_train_idx, u_val_idx])\n",
    "        v_train_idx = np.hstack([v_train_idx, v_val_idx])\n",
    "        train_labels = np.hstack([train_labels, val_labels])\n",
    "        # for adjacency matrix construction\n",
    "        train_idx = np.hstack([train_idx, val_idx])\n",
    "\n",
    "    # make training adjacency matrix\n",
    "    rating_mx_train = np.zeros(num_users * num_items, dtype=np.float32)\n",
    "    rating_mx_train[train_idx] = labels[train_idx].astype(np.float32) + 1.\n",
    "    rating_mx_train = sp.csr_matrix(rating_mx_train.reshape(num_users, num_items))\n",
    "\n",
    "    class_values = np.sort(np.unique(ratings))\n",
    "\n",
    "\n",
    "    item_df = pd.read_csv('../../underexpose_train/underexpose_item_feat.csv', header=None)\n",
    "\n",
    "    item_df[0] = item_df[0].apply(lambda x: int(x))\n",
    "    item_df[1] = item_df[1].apply(lambda x: float(x.strip('[')))\n",
    "    item_df[128] = item_df[128].apply(lambda x: float(x.strip(']')))\n",
    "    item_df[129] = item_df[129].apply(lambda x: float(x.strip('[')))\n",
    "    item_df[256] = item_df[256].apply(lambda x: float(x.strip(']')))\n",
    "\n",
    "    item_df = item_df.rename(columns = {0:'item_id'})\n",
    "\n",
    "    for col in range(1, len(item_df.columns)):\n",
    "        item_df[col] = item_df[col].values/item_df[col].values.max()\n",
    "\n",
    "    def check(v_id):\n",
    "        if v_id in v_dict.keys():\n",
    "            return v_dict[v_id]\n",
    "        return False\n",
    "\n",
    "    num_feats = len(item_df.columns) - 1\n",
    "    v_features = np.zeros((num_items, num_feats), dtype=np.float32)\n",
    "\n",
    "    sorted_by_value = item_df.item_id.apply(lambda v_id: check(v_id)).sort_values(ascending=True)[60678:]\n",
    "    dataframe_index = list(sorted_by_value.index)\n",
    "    matrix_index = list(sorted_by_value.values)\n",
    "\n",
    "    v_features[matrix_index] = item_df.iloc[dataframe_index][list(item_df.columns)[1:]].values\n",
    "\n",
    "    user_df = pd.read_csv('../../underexpose_train/underexpose_user_feat.csv', header=None)\n",
    "\n",
    "    user_df.columns = ['user_id', 'user_age_level', 'user_gender', 'user_city_level']\n",
    "    user_df.fillna(0,inplace=True)\n",
    "\n",
    "    user_age_dict = {f:i for i,f in enumerate(np.sort(user_df['user_age_level'].unique()))}\n",
    "    user_gender_dict = {0:9, 'F':10, 'M':11}\n",
    "    user_city_dict = {f:i for i,f in enumerate(np.sort(user_df['user_city_level'].unique()), start=12)}\n",
    "\n",
    "    num_feats = len(user_age_dict) + len(user_gender_dict) + len(user_city_dict)\n",
    "\n",
    "    u_features = np.zeros((num_users, num_feats), dtype=np.float32)\n",
    "    for _, row in user_df.iterrows():\n",
    "        u_id = row['user_id']\n",
    "        if u_id in u_dict.keys():\n",
    "            # age\n",
    "            u_features[u_dict[u_id], user_age_dict[row['user_age_level']]] = 1\n",
    "            # gender\n",
    "            u_features[u_dict[u_id], user_gender_dict[row['user_gender']]] = 1\n",
    "            # occupation\n",
    "            u_features[u_dict[u_id], user_city_dict[row['user_city_level']]] = 1.\n",
    "\n",
    "    u_features = sp.csr_matrix(u_features)\n",
    "    v_features = sp.csr_matrix(v_features)\n",
    "\n",
    "    print(\"User features shape: \"+str(u_features.shape))\n",
    "    print(\"Item features shape: \"+str(v_features.shape))\n",
    "\n",
    "    return u_features, v_features, rating_mx_train, train_labels, u_train_idx, v_train_idx, \\\n",
    "        val_labels, u_val_idx, v_val_idx, test_labels, u_test_idx, v_test_idx, class_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "DATASET = args['dataset']\n",
    "DATASEED = args['data_seed']\n",
    "NB_EPOCH = args['epochs']\n",
    "DO = args['dropout']\n",
    "HIDDEN = args['hidden']\n",
    "FEATHIDDEN = args['feat_hidden']\n",
    "BASES = args['num_basis_functions']\n",
    "LR = args['learning_rate']\n",
    "WRITESUMMARY = args['write_summary']\n",
    "SUMMARIESDIR = args['summaries_dir']\n",
    "FEATURES = args['features']\n",
    "SYM = args['norm_symmetric']\n",
    "TESTING = args['testing']\n",
    "ACCUM = args['accumulation']\n",
    "\n",
    "SELFCONNECTIONS = False\n",
    "SPLITFROMFILE = True\n",
    "VERBOSE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:28: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:30: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User features shape: (21360, 19)\n",
      "Item features shape: (51914, 256)\n",
      "Normalizing feature vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/ipykernel_launcher.py:311: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "NUMCLASSES = 1\n",
    "\n",
    "# Splitting dataset in training, validation and test set\n",
    "\n",
    "# if DATASET == 'ml_1m' or DATASET == 'ml_10m':\n",
    "#     if FEATURES:\n",
    "#         datasplit_path = 'data/' + DATASET + '/withfeatures_split_seed' + str(DATASEED) + '.pickle'\n",
    "#     else:\n",
    "#         datasplit_path = 'data/' + DATASET + '/split_seed' + str(DATASEED) + '.pickle'\n",
    "# elif FEATURES:\n",
    "#     datasplit_path = 'data/' + DATASET + '/withfeatures.pickle'\n",
    "# else:\n",
    "#     datasplit_path = 'data/' + DATASET + '/nofeatures.pickle'\n",
    "\n",
    "u_features, v_features, adj_train, train_labels, train_u_indices, train_v_indices, \\\n",
    "    val_labels, val_u_indices, val_v_indices, test_labels, \\\n",
    "    test_u_indices, test_v_indices, class_values = load_ourdata(now_phase = 1, testing = True)\n",
    "\n",
    "num_users, num_items = adj_train.shape\n",
    "\n",
    "num_side_features = 0\n",
    "\n",
    "# feature loading\n",
    "if not FEATURES:\n",
    "    u_features = sp.identity(num_users, format='csr')\n",
    "    v_features = sp.identity(num_items, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(u_features, v_features)\n",
    "\n",
    "elif FEATURES and u_features is not None and v_features is not None:\n",
    "    # use features as side information and node_id's as node input features\n",
    "\n",
    "    print(\"Normalizing feature vectors...\")\n",
    "    u_features_side = normalize_features(u_features)\n",
    "    v_features_side = normalize_features(v_features)\n",
    "\n",
    "    u_features_side, v_features_side = preprocess_user_item_features(u_features_side, v_features_side)\n",
    "\n",
    "    u_features_side = np.array(u_features_side.todense(), dtype=np.float32)\n",
    "    v_features_side = np.array(v_features_side.todense(), dtype=np.float32)\n",
    "\n",
    "    num_side_features = u_features_side.shape[1]\n",
    "\n",
    "    # node id's for node input features\n",
    "    id_csr_v = sp.identity(num_items, format='csr')\n",
    "    id_csr_u = sp.identity(num_users, format='csr')\n",
    "\n",
    "    u_features, v_features = preprocess_user_item_features(id_csr_u, id_csr_v)\n",
    "\n",
    "else:\n",
    "    raise ValueError('Features flag is set to true but no features are loaded from dataset ' + DATASET)\n",
    "\n",
    "\n",
    "# global normalization\n",
    "support = []\n",
    "support_t = []\n",
    "adj_train_int = sp.csr_matrix(adj_train, dtype=np.int32)\n",
    "\n",
    "for i in range(NUMCLASSES):\n",
    "    # build individual binary rating matrices (supports) for each rating\n",
    "    support_unnormalized = sp.csr_matrix(adj_train_int == i + 1, dtype=np.float32)\n",
    "\n",
    "    if support_unnormalized.nnz == 0 and DATASET != 'yahoo_music':\n",
    "        # yahoo music has dataset split with not all ratings types present in training set.\n",
    "        # this produces empty adjacency matrices for these ratings.\n",
    "        sys.exit('ERROR: normalized bipartite adjacency matrix has only zero entries!!!!!')\n",
    "\n",
    "    support_unnormalized_transpose = support_unnormalized.T\n",
    "    support.append(support_unnormalized)\n",
    "    support_t.append(support_unnormalized_transpose)\n",
    "\n",
    "\n",
    "support = globally_normalize_bipartite_adjacency(support, symmetric=SYM)\n",
    "support_t = globally_normalize_bipartite_adjacency(support_t, symmetric=SYM)\n",
    "\n",
    "if SELFCONNECTIONS:\n",
    "    support.append(sp.identity(u_features.shape[0], format='csr'))\n",
    "    support_t.append(sp.identity(v_features.shape[0], format='csr'))\n",
    "\n",
    "num_support = len(support)\n",
    "support = sp.hstack(support, format='csr')\n",
    "support_t = sp.hstack(support_t, format='csr')\n",
    "\n",
    "if ACCUM == 'stack':\n",
    "    div = HIDDEN[0] // num_support\n",
    "    if HIDDEN[0] % num_support != 0:\n",
    "        print(\"\"\"\\nWARNING: HIDDEN[0] (=%d) of stack layer is adjusted to %d such that\n",
    "                  it can be evenly split in %d splits.\\n\"\"\" % (HIDDEN[0], num_support * div, num_support))\n",
    "    HIDDEN[0] = num_support * div\n",
    "\n",
    "# Collect all user and item nodes for test set\n",
    "test_u = list(set(test_u_indices))\n",
    "test_v = list(set(test_v_indices))\n",
    "test_u_dict = {n: i for i, n in enumerate(test_u)}\n",
    "test_v_dict = {n: i for i, n in enumerate(test_v)}\n",
    "\n",
    "test_u_indices = np.array([test_u_dict[o] for o in test_u_indices])\n",
    "test_v_indices = np.array([test_v_dict[o] for o in test_v_indices])\n",
    "\n",
    "test_support = support[np.array(test_u)]\n",
    "test_support_t = support_t[np.array(test_v)]\n",
    "\n",
    "# Collect all user and item nodes for validation set\n",
    "val_u = list(set(val_u_indices))\n",
    "val_v = list(set(val_v_indices))\n",
    "val_u_dict = {n: i for i, n in enumerate(val_u)}\n",
    "val_v_dict = {n: i for i, n in enumerate(val_v)}\n",
    "\n",
    "val_u_indices = np.array([val_u_dict[o] for o in val_u_indices])\n",
    "val_v_indices = np.array([val_v_dict[o] for o in val_v_indices])\n",
    "\n",
    "val_support = support[np.array(val_u)]\n",
    "val_support_t = support_t[np.array(val_v)]\n",
    "\n",
    "# Collect all user and item nodes for train set\n",
    "train_u = list(set(train_u_indices))\n",
    "train_v = list(set(train_v_indices))\n",
    "train_u_dict = {n: i for i, n in enumerate(train_u)}\n",
    "train_v_dict = {n: i for i, n in enumerate(train_v)}\n",
    "\n",
    "train_u_indices = np.array([train_u_dict[o] for o in train_u_indices])\n",
    "train_v_indices = np.array([train_v_dict[o] for o in train_v_indices])\n",
    "\n",
    "train_support = support[np.array(train_u)]\n",
    "train_support_t = support_t[np.array(train_v)]\n",
    "\n",
    "# features as side info\n",
    "if FEATURES:\n",
    "    test_u_features_side = u_features_side[np.array(test_u)]\n",
    "    test_v_features_side = v_features_side[np.array(test_v)]\n",
    "\n",
    "    val_u_features_side = u_features_side[np.array(val_u)]\n",
    "    val_v_features_side = v_features_side[np.array(val_v)]\n",
    "\n",
    "    train_u_features_side = u_features_side[np.array(train_u)]\n",
    "    train_v_features_side = v_features_side[np.array(train_v)]\n",
    "\n",
    "else:\n",
    "    test_u_features_side = None\n",
    "    test_v_features_side = None\n",
    "\n",
    "    val_u_features_side = None\n",
    "    val_v_features_side = None\n",
    "\n",
    "    train_u_features_side = None\n",
    "    train_v_features_side = None\n",
    "\n",
    "placeholders = {\n",
    "    'u_features': tf.sparse_placeholder(tf.float32, shape=np.array(u_features.shape, dtype=np.int64)),\n",
    "    'v_features': tf.sparse_placeholder(tf.float32, shape=np.array(v_features.shape, dtype=np.int64)),\n",
    "    'u_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'v_features_nonzero': tf.placeholder(tf.int32, shape=()),\n",
    "    'labels': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'u_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "    'v_features_side': tf.placeholder(tf.float32, shape=(None, num_side_features)),\n",
    "\n",
    "    'user_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "    'item_indices': tf.placeholder(tf.int32, shape=(None,)),\n",
    "\n",
    "    'class_values': tf.placeholder(tf.float32, shape=class_values.shape),\n",
    "\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'weight_decay': tf.placeholder_with_default(0., shape=()),\n",
    "\n",
    "    'support': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "    'support_t': tf.sparse_placeholder(tf.float32, shape=(None, None)),\n",
    "}\n",
    "\n",
    "# create model\n",
    "if FEATURES:\n",
    "    model = RecommenderSideInfoGAE(placeholders,\n",
    "                                   input_dim=u_features.shape[1],\n",
    "                                   feat_hidden_dim=FEATHIDDEN,\n",
    "                                   num_classes=NUMCLASSES,\n",
    "                                   num_support=num_support,\n",
    "                                   self_connections=SELFCONNECTIONS,\n",
    "                                   num_basis_functions=BASES,\n",
    "                                   hidden=HIDDEN,\n",
    "                                   num_users=num_users,\n",
    "                                   num_items=num_items,\n",
    "                                   accum=ACCUM,\n",
    "                                   learning_rate=LR,\n",
    "                                   num_side_features=num_side_features,\n",
    "                                   logging=True)\n",
    "\n",
    "# Convert sparse placeholders to tuples to construct feed_dict\n",
    "test_support = sparse_to_tuple(test_support)\n",
    "test_support_t = sparse_to_tuple(test_support_t)\n",
    "\n",
    "val_support = sparse_to_tuple(val_support)\n",
    "val_support_t = sparse_to_tuple(val_support_t)\n",
    "\n",
    "train_support = sparse_to_tuple(train_support)\n",
    "train_support_t = sparse_to_tuple(train_support_t)\n",
    "\n",
    "u_features = sparse_to_tuple(u_features)\n",
    "v_features = sparse_to_tuple(v_features)\n",
    "assert u_features[2][1] == v_features[2][1], 'Number of features of users and items must be the same!'\n",
    "\n",
    "num_features = u_features[2][1]\n",
    "u_features_nonzero = u_features[1].shape[0]\n",
    "v_features_nonzero = v_features[1].shape[0]\n",
    "\n",
    "# Feed_dicts for validation and test set stay constant over different update steps\n",
    "train_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                      v_features_nonzero, train_support, train_support_t,\n",
    "                                      train_labels, train_u_indices, train_v_indices, class_values, DO,\n",
    "                                      train_u_features_side, train_v_features_side)\n",
    "# No dropout for validation and test runs\n",
    "val_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                    v_features_nonzero, val_support, val_support_t,\n",
    "                                    val_labels, val_u_indices, val_v_indices, class_values, 0.,\n",
    "                                    val_u_features_side, val_v_features_side)\n",
    "\n",
    "test_feed_dict = construct_feed_dict(placeholders, u_features, v_features, u_features_nonzero,\n",
    "                                     v_features_nonzero, test_support, test_support_t,\n",
    "                                     test_labels, test_u_indices, test_v_indices, class_values, 0.,\n",
    "                                     test_u_features_side, test_v_features_side)\n",
    "\n",
    "\n",
    "# Collect all variables to be logged into summary\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if WRITESUMMARY:\n",
    "    train_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/train', sess.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter(SUMMARIESDIR + '/val')\n",
    "else:\n",
    "    train_summary_writer = None\n",
    "    val_summary_writer = None\n",
    "\n",
    "best_val_score = np.inf\n",
    "best_val_loss = np.inf\n",
    "best_epoch = 0\n",
    "wait = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[*] Epoch: 0001 train_loss= 0.00000 train_rmse= 0.00000 val_loss= 0.00000 val_rmse= 0.00000 \t\ttime= 3.11610\n",
      "[*] Epoch: 0002 train_loss= 0.00000 train_rmse= 0.00000 val_loss= 0.00000 val_rmse= 0.00000 \t\ttime= 3.05945\n",
      "[*] Epoch: 0003 train_loss= 0.00000 train_rmse= 0.00000 val_loss= 0.00000 val_rmse= 0.00000 \t\ttime= 3.13045\n",
      "[*] Epoch: 0004 train_loss= 0.00000 train_rmse= 0.00000 val_loss= 0.00000 val_rmse= 0.00000 \t\ttime= 3.30949\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-dd95caa400c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# outs = sess.run([model.opt_op, model.loss, model.rmse], feed_dict=train_feed_dict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# with exponential moving averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_avg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/guojy/anaconda3/envs/pt1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('Training...')\n",
    "\n",
    "for epoch in range(NB_EPOCH):\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    # Run single weight update\n",
    "    # outs = sess.run([model.opt_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "    # with exponential moving averages\n",
    "    outs = sess.run([model.training_op, model.loss, model.rmse], feed_dict=train_feed_dict)\n",
    "\n",
    "    train_avg_loss = outs[1]\n",
    "    train_rmse = outs[2]\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "    if VERBOSE:\n",
    "        print(\"[*] Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(train_avg_loss),\n",
    "              \"train_rmse=\", \"{:.5f}\".format(train_rmse),\n",
    "              \"val_loss=\", \"{:.5f}\".format(val_avg_loss),\n",
    "              \"val_rmse=\", \"{:.5f}\".format(val_rmse),\n",
    "              \"\\t\\ttime=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if val_rmse < best_val_score:\n",
    "        best_val_score = val_rmse\n",
    "        best_epoch = epoch\n",
    "\n",
    "    if epoch % 20 == 0 and WRITESUMMARY:\n",
    "        # Train set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=train_feed_dict)\n",
    "        train_summary_writer.add_summary(summary, epoch)\n",
    "        train_summary_writer.flush()\n",
    "\n",
    "        # Validation set summary\n",
    "        summary = sess.run(merged_summary, feed_dict=val_feed_dict)\n",
    "        val_summary_writer.add_summary(summary, epoch)\n",
    "        val_summary_writer.flush()\n",
    "\n",
    "    if epoch % 100 == 0 and epoch > 1000 and not TESTING and False:\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(sess, \"tmp/%s_seed%d.ckpt\" % (model.name, DATASEED), global_step=model.global_step)\n",
    "\n",
    "        # load polyak averages\n",
    "        variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "        val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "\n",
    "        print('polyak val loss = ', val_avg_loss)\n",
    "        print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "        # Load back normal variables\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path)\n",
    "\n",
    "\n",
    "# store model including exponential moving averages\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, \"tmp/%s.ckpt\" % model.name, global_step=model.global_step)\n",
    "\n",
    "\n",
    "if VERBOSE:\n",
    "    print(\"\\nOptimization Finished!\")\n",
    "    print('best validation score =', best_val_score, 'at iteration', best_epoch)\n",
    "\n",
    "\n",
    "if TESTING:\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('test loss = ', test_avg_loss)\n",
    "    print('test rmse = ', test_rmse)\n",
    "\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    test_avg_loss, test_rmse = sess.run([model.loss, model.rmse], feed_dict=test_feed_dict)\n",
    "    print('polyak test loss = ', test_avg_loss)\n",
    "    print('polyak test rmse = ', test_rmse)\n",
    "\n",
    "else:\n",
    "    # restore with polyak averages of parameters\n",
    "    variables_to_restore = model.variable_averages.variables_to_restore()\n",
    "    saver = tf.train.Saver(variables_to_restore)\n",
    "    saver.restore(sess, save_path)\n",
    "\n",
    "    val_avg_loss, val_rmse = sess.run([model.loss, model.rmse], feed_dict=val_feed_dict)\n",
    "    print('polyak val loss = ', val_avg_loss)\n",
    "    print('polyak val rmse = ', val_rmse)\n",
    "\n",
    "print('\\nSETTINGS:\\n')\n",
    "for key, val in sorted(vars(ap.parse_args()).items()):\n",
    "    print(key, val)\n",
    "\n",
    "print('global seed = ', seed)\n",
    "\n",
    "# For parsing results from file\n",
    "results = vars(ap.parse_args()).copy()\n",
    "results.update({'best_val_score': float(best_val_score), 'best_epoch': best_epoch})\n",
    "print(json.dumps(results))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = '\\t'\n",
    "# Check if files exist and download otherwise\n",
    "files = ['/u1.base', '/u1.test', '/u.item', '/u.user']\n",
    "dataset = 'ml_100k'\n",
    "fname = dataset\n",
    "data_dir = 'data/' + fname\n",
    "dtypes = {\n",
    "    'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "    'ratings': np.float32, 'timestamp': np.float64}\n",
    "filename_train = 'data/' + dataset + '/u1.base'\n",
    "filename_test = 'data/' + dataset + '/u1.test'\n",
    "data_train = pd.read_csv(\n",
    "    filename_train, sep=sep, header=None,\n",
    "    names=['u_nodes', 'v_nodes', 'ratings', 'timestamp'], dtype=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phase: 0\n",
      "phase: 1\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../underexpose_train'  \n",
    "test_path = '../../underexpose_test'  \n",
    "recom_item = []  \n",
    "dtypes = {\n",
    "'u_nodes': np.int32, 'v_nodes': np.int32,\n",
    "'ratings': np.float32, 'timestamp': np.float64}\n",
    "data_train = pd.DataFrame()  \n",
    "data_test = pd.DataFrame()\n",
    "for c in range(2):  \n",
    "    print('phase:', c)  \n",
    "    click_train = pd.read_csv(train_path + '/underexpose_train_click-{}-real.csv'.format(c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    click_test = pd.read_csv(test_path + '/underexpose_test_click-{}.csv'.format(c,c), header=None,  names=['user_id', 'item_id', 'time'])  \n",
    "    answer = pd.read_csv(test_path + '/underexpose_test_qtime_with_answer-{}.csv'.format(c), header=None, names=['user_id', 'item_id', 'time'])\n",
    "    all_click = click_train.append(click_test)  \n",
    "    data_train =data_train.append(all_click)  \n",
    "    #data_train = data_train.drop_duplicates(subset=['user_id','item_id','time'],keep='last')\n",
    "    #data_train = data_train.sort_values('time')\n",
    "    data_test = data_test.append(answer)\n",
    "data_train = data_train.reset_index().drop(columns=['index'], axis=1)\n",
    "data_train.insert(loc=2, column='ratings', value=np.ones(len(data_train)))\n",
    "data_train.columns = ['u_nodes','v_nodes','ratings','timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21360"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train.u_nodes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51914"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train.v_nodes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
